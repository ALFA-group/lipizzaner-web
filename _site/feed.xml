<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/lipizzaner-web/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/lipizzaner-web/" rel="alternate" type="text/html" /><updated>2020-07-10T11:14:14-04:00</updated><id>http://localhost:4000/lipizzaner-web/feed.xml</id><title type="html">Lipizzaner</title><subtitle>Lipizzaner is a framework to efficiently train generative adversarial networks (GANs).</subtitle><entry><title type="html">Lipizzaner development environment setup</title><link href="http://localhost:4000/lipizzaner-web/lipi-install/" rel="alternate" type="text/html" title="Lipizzaner development environment setup" /><published>2019-01-24T00:00:00-05:00</published><updated>2019-01-24T00:00:00-05:00</updated><id>http://localhost:4000/lipizzaner-web/lipi-install</id><content type="html" xml:base="http://localhost:4000/lipizzaner-web/lipi-install/">&lt;p&gt;Lipizzaner source code is freely available in GitHub. Thus, to start one has just to clone the master branch.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/ALFA-group/lipizzaner-gan.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, there are two different options to setup the environment to use our framework.&lt;/p&gt;
&lt;h5 id=&quot;conda&quot;&gt;Conda&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda env create -f ./src/helper_files/environment.yml
source activate lipizzaner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;pip&quot;&gt;Pip&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install -r ./src/helper_files/requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>jamal</name></author><category term="tutorial" /><category term="setup" /><summary type="html">Lipizzaner source code is freely available in GitHub. Thus, to start one has just to clone the master branch.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/lipizzaner-web/assets/images/tutorials/installer.png" /></entry><entry><title type="html">Lipizzaner: A System That Scales Robust Generative Adversarial Network Training</title><link href="http://localhost:4000/lipizzaner-web/lipi-home/" rel="alternate" type="text/html" title="Lipizzaner: A System That Scales Robust Generative Adversarial Network Training" /><published>2018-01-01T00:00:00-05:00</published><updated>2018-01-01T00:00:00-05:00</updated><id>http://localhost:4000/lipizzaner-web/lipi-home</id><content type="html" xml:base="http://localhost:4000/lipizzaner-web/lipi-home/">&lt;p&gt;Over the last few years, researchers and practitioners have found in &lt;strong&gt;Generative Adversarial Networks&lt;/strong&gt; (GANs) a tool to address several challenging machine learning problems.
Most of these problems are related to generative machine learning, but we can find others like semi-supervised learning tacked to create classifiers by using few labeled data samples.
Despite its success, GAN training presents several limitiations or pathologies. The most commonly observed ones are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;mode collapse&lt;/em&gt;: the generator can only generate one mode of the distribution&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;non-convergence&lt;/em&gt;: the algorithm does not find an equilibrium, and&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;vanishing gradients&lt;/em&gt;: there are no training gradients, i.e. one adversary is too strong/weak.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At &lt;strong&gt;&lt;a href=&quot;http://alfagroup.csail.mit.edu/&quot;&gt;Anyscale Learning For All (ALFA)&lt;/a&gt;&lt;/strong&gt;, we have designed and developed &lt;strong&gt;Lipizzaner&lt;/strong&gt; framework, which applies &lt;strong&gt;spatially distributed co-evolution&lt;/strong&gt; to provide &lt;strong&gt;resilient and robust GAN training&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The main advantages of using Lipizzaner to train GANs are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fast convergence&lt;/strong&gt; due to gradient-based steps&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved convergence&lt;/strong&gt; due to hyperparameter evolution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Diverse sample generation&lt;/strong&gt; due to mixture evolution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; due to spatial distribution topology and asynchronous parallelism&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness and resilience&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;iframe style=&quot;width:100%;&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6S0sqRWEsqY?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;This website will include plenty of information allowing the users take advantage of this framework to deal with their own problems.&lt;/p&gt;

&lt;p&gt;If you have any coments, do not hesitate to contact us.&lt;/p&gt;

&lt;h3 id=&quot;comming-soon&quot;&gt;Comming soon&lt;/h3&gt;
&lt;p&gt;We are launching a new release, &lt;strong&gt;Lipizzaner 2.0&lt;/strong&gt;, you will ride a much more powerful horse. This new release incorporates a set of features that allows you to easily extend your own distributed Co-evolutionary GAN training. Among others it includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semi-Supervised learning&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;New state-of-the-art &lt;strong&gt;data sets&lt;/strong&gt;, including Covid-19 X-Rays images&lt;/li&gt;
  &lt;li&gt;New &lt;strong&gt;architectures&lt;/strong&gt;, e.g., MLP, CNN, and RNN with multiple sizes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data dieting&lt;/strong&gt;:&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-selection in the cells&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Extra &lt;strong&gt;ensemble optimization&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;and much more&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;funding&quot;&gt;Funding&lt;/h3&gt;
&lt;p&gt;This research was partially funded by the Systems that Learn Initiative at MIT CSAIL.
&lt;a href=&quot;https://jamal.es&quot;&gt;Jamal Toutouh&lt;/a&gt; was partially funded by the European Union’s Horizon 2020 research and innovation program under the
        Marie Skłodowska-Curie grant agreement No 799078.&lt;/p&gt;</content><author><name>jamal</name></author><category term="sticky" /><summary type="html">Over the last few years, researchers and practitioners have found in Generative Adversarial Networks (GANs) a tool to address several challenging machine learning problems. Most of these problems are related to generative machine learning, but we can find others like semi-supervised learning tacked to create classifiers by using few labeled data samples. Despite its success, GAN training presents several limitiations or pathologies. The most commonly observed ones are: mode collapse: the generator can only generate one mode of the distribution non-convergence: the algorithm does not find an equilibrium, and vanishing gradients: there are no training gradients, i.e. one adversary is too strong/weak.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/lipizzaner-web/assets/images/alfa-logo.png" /></entry></feed>